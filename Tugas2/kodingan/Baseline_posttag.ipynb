{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca data training\n",
    "def read_file_init_table(fname):\n",
    "    word_tag = {}\n",
    "    words = dict()\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    idx_line = 0\n",
    "    \n",
    "    while idx_line < len(content):\n",
    "        while not content[idx_line].startswith('</kalimat'):\n",
    "            if  not content[idx_line].startswith('<kalimat'):\n",
    "                content_part = content[idx_line].split('\\t')\n",
    "                content_part[0] = content_part[0].lower()\n",
    "                content_part[1] = content_part[1].lower()\n",
    "                current_word_tag = content_part[0]+','+content_part[1]\n",
    "                if current_word_tag in word_tag:\n",
    "                    word_tag[current_word_tag] += 1\n",
    "                else: \n",
    "                    word_tag[current_word_tag] = 1\n",
    "                if content_part[0] in words:\n",
    "                    if content_part[1] not in words[content_part[0]]:\n",
    "                        words[content_part[0]].append(content_part[1])\n",
    "                else:\n",
    "                    words[content_part[0]] = [content_part[1]]                   \n",
    "            idx_line = idx_line + 1\n",
    "        idx_line = idx_line+1 \n",
    "    return word_tag,words\n",
    "\n",
    "word_tag,words= read_file_init_table('data_training.txt')\n",
    "#print(\"WORD TAG :\")\n",
    "#print(word_tag,\"\\n\")\n",
    "#print(\"words\")\n",
    "#print(words,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baca data test\n",
    "def data_tes(file_name):\n",
    "    sentence = []\n",
    "    sentences = []\n",
    "    true_result = []\n",
    "    true_results = []\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "        \n",
    "    idx_line = 0\n",
    "    \n",
    "    while idx_line < len(content):\n",
    "        if not content[idx_line].startswith('</kalimat'):\n",
    "            if  not content[idx_line].startswith('<kalimat'):\n",
    "                kata = content[idx_line].split('\\t')\n",
    "                kata[0] = kata[0].lower()\n",
    "                kata[1] = kata[1].lower()\n",
    "                true_result.append(kata[0]+','+kata[1])\n",
    "                sentence.append(kata[0]) #ambil kata tanpa tag\n",
    "        else:\n",
    "            true_results.append(true_result)\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            true_result = []\n",
    "        idx_line = idx_line + 1\n",
    "    return sentences,true_results\n",
    "sentences, true_results = data_tes('data_tes.txt')\n",
    "#print(sentences,\"\\n\")\n",
    "#print(true_results,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pemberian posttag dengan metode baseline\n",
    "def baseline(sentences,word_tag,words):\n",
    "    result = []\n",
    "    results = []\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            best = 0\n",
    "            if sentences[i][j] in words:\n",
    "                for k in range(len(words[sentences[i][j]])):\n",
    "                    kata = sentences[i][j]+\",\"+words[sentences[i][j]][k]\n",
    "                    if kata in word_tag:\n",
    "                        if word_tag[kata] > best:\n",
    "                            best = word_tag[kata]\n",
    "                            tag = words[sentences[i][j]][k]\n",
    " \n",
    "                result.append(sentences[i][j]+','+tag)\n",
    "            else:\n",
    "                result.append(sentences[i][j]+',nn')\n",
    "        results.append(result)\n",
    "        result = []\n",
    "    return results\n",
    "    \n",
    "results = baseline(sentences,word_tag,words)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membandingkan hasil tag yang didapat dengan tag yang sebenarnya\n",
    "def cek(results,true_results):\n",
    "#    sama = 0\n",
    "    jml_kata = 0\n",
    "    counter = 0\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(results[i])):\n",
    "#             print(results[i][j],\" : \",true_results[i][j])\n",
    "            if results[i][j] == true_results[i][j]:\n",
    "                counter= counter+1\n",
    "            jml_kata +=1\n",
    "\n",
    "    return counter,jml_kata\n",
    "\n",
    "counter, jml_kata =cek(results,true_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi Baseline Posttag :  87.97595190380761 %\n"
     ]
    }
   ],
   "source": [
    "# hitung akurasi ketepatan posttag baseline\n",
    "def akurasi(counter,jml_kata):\n",
    "    acc = (counter / jml_kata)*100\n",
    "    return acc\n",
    "\n",
    "acc = akurasi(counter,jml_kata)\n",
    "print(\"Akurasi Baseline Posttag : \",acc,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
